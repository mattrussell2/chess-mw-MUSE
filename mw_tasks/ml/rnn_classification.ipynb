{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from library import *\n",
    "\n",
    "import itertools\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try chess classification on its own first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2242, 28)\n",
      "(1680, 20) - (1680,) - (562, 20) - (562,)\n",
      "[LightGBM] [Info] Number of positive: 1096, number of negative: 584\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 1680, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652381 -> initscore=0.629521\n",
      "[LightGBM] [Info] Start training from score 0.629521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrussell/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    }
   ],
   "source": [
    "chess_df = pd.read_csv('CHESS_notch_bp_avg_mastoid_annotated_ica_fft.csv')\n",
    "chess_df['group_id'] = chess_df['pid'] + '_' + chess_df['blockid'].astype(str)\n",
    "chess_df = chess_df[(chess_df['elo-bin'] == 'lo') | (chess_df['elo-bin'] == 'hi')]\n",
    "chess_df['workload'] = chess_df['elo-bin'].apply(lambda x: 0 if x == 'lo' else 1)\n",
    "#chess_df = chess_df.drop(columns=['elo', 'solved', 'blockid', 'sub-block-id', 'elo-bin', 'pid', 'trial_id'])\n",
    "chess_freq = produce_freq_bands(chess_df)\n",
    "print(chess_freq.shape)\n",
    "chess_freq\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(chess_freq)\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train, X_test)\n",
    "results = classify(X_train_scaled, X_test_scaled, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def split_data(df):\n",
    "    # Initial split based on pid\n",
    "    X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "    # Iterate over each unique pid\n",
    "    for unique_pid in df['pid'].unique():\n",
    "\n",
    "        # Filter data for the current pid\n",
    "        pid_data = df[df['pid'] == unique_pid]\n",
    "        \n",
    "        X_pid = pid_data.drop(columns=[key for key in pid_data.keys() if 'ch_' not in key])\n",
    "        y_pid = pid_data['workload']\n",
    "        group_pid = pid_data['group_id']\n",
    "        \n",
    "        # Use GroupShuffleSplit to split data based on the group for the current pid\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "        train_idx, test_idx = next(gss.split(X_pid, y_pid, groups=group_pid))\n",
    "        \n",
    "        X_train_list.append(X_pid.iloc[train_idx])\n",
    "        X_test_list.append(X_pid.iloc[test_idx])\n",
    "        y_train_list.append(y_pid.iloc[train_idx])\n",
    "        y_test_list.append(y_pid.iloc[test_idx])\n",
    "\n",
    "    # Concatenate the results to get the final train and test sets\n",
    "    X_train = pd.concat(X_train_list)\n",
    "    X_test = pd.concat(X_test_list)\n",
    "    y_train = pd.concat(y_train_list)\n",
    "    y_test = pd.concat(y_test_list)\n",
    "\n",
    "    print(f\"{X_train.shape} - {y_train.shape} - {X_test.shape} - {y_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps.\n",
    "\n",
    "# wl_df = wl_df.drop(columns=['time'])\n",
    "# wl_df = wl_df[(wl_df['trialtype'] == 'nback')] #| (wl_df['trialtype'] == 'stroop')] #(wl_df['trialtype'] == 'rotation') | \n",
    "\n",
    "# wl_df.loc[:, 'group_id'] = wl_df['filename']\n",
    "# wl_df = wl_df.drop(columns=['filename', 'correct', 'trial_id', 'trialtype'])\n",
    "\n",
    "# # # -1 trials are not used [first three for nback and/or rest, etc.]\n",
    "# wl_df = wl_df[wl_df['workload'] != '-1']\n",
    "\n",
    "# # # 'True'/'False' is used by stroop, whereas 0,1,2,3 are used for nback and rotation\n",
    "# # wl_df['workload'] = wl_df['workload'].apply(lambda x: '3.0' if x == 'True' else '0.0' if x == \"False\" else x)\n",
    "# wl_df['workload'] = wl_df['workload'].astype(float).astype(int)\n",
    "\n",
    "# for i, row in wl_df.iterrows():\n",
    "#     if row['workload'] == 1:        \n",
    "#         wl_df.loc[i, 'workload'] = 0\n",
    "#     elif row['workload'] == 2 or row['workload'] == 3:\n",
    "#         wl_df.loc[i, 'workload'] = 1\n",
    "# wl_df\n",
    "# wl_freq = produce_freq_bands(wl_df)\n",
    "\n",
    "# print(wl_freq.shape)\n",
    "# wl_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2346, 4, 930), (587, 4, 930))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wl_df = pd.read_csv('workload_raw_joined_preprocessed.csv', dtype={'pid':str, 'group_id':str, 'workload':int})\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reshape the data\n",
    "eeg_channels = ['probe-0', 'probe-1', 'probe-2', 'probe-3']\n",
    "data_array = []\n",
    "epochs = wl_df['epoch'].unique()\n",
    "\n",
    "for epoch in epochs:\n",
    "    epoch_data = wl_df[wl_df['epoch'] == epoch][eeg_channels].values.T\n",
    "    data_array.append(epoch_data)\n",
    "\n",
    "data_array = np.array(data_array)\n",
    "\n",
    "# Split into test and train datasets\n",
    "# We'll use the group_id to ensure that the same group_id doesn't appear in both train and test datasets\n",
    "# Stratify based on 'pid' to ensure even distribution\n",
    "train_data, test_data, train_meta, test_meta = train_test_split(\n",
    "    data_array, \n",
    "    wl_df.drop_duplicates(subset='epoch')[['workload', 'pid', 'group_id']], \n",
    "    test_size=0.2, \n",
    "    stratify=wl_df.drop_duplicates(subset='epoch')['pid'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data.shape, test_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the data to 2D: (number of samples, number of features)\n",
    "train_data_2d = train_data.reshape(train_data.shape[0], -1)\n",
    "test_data_2d = test_data.reshape(test_data.shape[0], -1)\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_data_2d = scaler.fit_transform(train_data_2d)\n",
    "\n",
    "# Transform the test data\n",
    "test_data_2d = scaler.transform(test_data_2d)\n",
    "\n",
    "# Reshape the data back to 3D for EEGNet\n",
    "train_data = train_data_2d.reshape(train_data.shape)\n",
    "test_data = test_data_2d.reshape(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_meta['workload'].values\n",
    "test_labels = test_meta['workload'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.models import EEGNetv4\n",
    "\n",
    "n_channels, n_samples = 4, 930  # As per your data\n",
    "n_classes = 2  # 'workload' has two classes: 0 and 1\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_channels,\n",
    "    n_classes,\n",
    "    input_window_samples=n_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25438/4031289604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Convert data and labels to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Convert binary labels to one-hot encoded format\n",
    "train_labels_onehot = F.one_hot(train_labels_tensor.to(torch.int64), num_classes=2).float()\n",
    "test_labels_onehot = F.one_hot(test_labels_tensor.to(torch.int64), num_classes=2).float()\n",
    "\n",
    "\n",
    "# Set criterion and optimizer\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (simplified)\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(train_data_tensor)\n",
    "    loss = criterion(outputs, train_labels_onehot)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every few epochs (e.g., 10 epochs)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch [{epoch}/{n_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass on the test set\n",
    "test_outputs = model(test_data_tensor)\n",
    "\n",
    "# Convert logits to probabilities using sigmoid\n",
    "test_probs = torch.sigmoid(test_outputs)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "test_preds = (test_probs > 0.5).float()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (test_preds == test_labels_onehot).float().mean().item()\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2242, 83)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ch_0_Delta-mean</th>\n",
       "      <th>ch_0_Delta-kurt</th>\n",
       "      <th>ch_0_Delta-skew</th>\n",
       "      <th>ch_0_Delta-std</th>\n",
       "      <th>ch_0_Theta-mean</th>\n",
       "      <th>ch_0_Theta-kurt</th>\n",
       "      <th>ch_0_Theta-skew</th>\n",
       "      <th>ch_0_Theta-std</th>\n",
       "      <th>ch_0_Alpha-mean</th>\n",
       "      <th>ch_0_Alpha-kurt</th>\n",
       "      <th>...</th>\n",
       "      <th>ch_3_Beta-kurt</th>\n",
       "      <th>ch_3_Beta-skew</th>\n",
       "      <th>ch_3_Beta-std</th>\n",
       "      <th>ch_3_Gamma-mean</th>\n",
       "      <th>ch_3_Gamma-kurt</th>\n",
       "      <th>ch_3_Gamma-skew</th>\n",
       "      <th>ch_3_Gamma-std</th>\n",
       "      <th>pid</th>\n",
       "      <th>group_id</th>\n",
       "      <th>workload</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390.720847</td>\n",
       "      <td>-1.310962</td>\n",
       "      <td>0.178668</td>\n",
       "      <td>152.372900</td>\n",
       "      <td>428.479043</td>\n",
       "      <td>-0.226652</td>\n",
       "      <td>-0.017858</td>\n",
       "      <td>155.792028</td>\n",
       "      <td>253.743533</td>\n",
       "      <td>-0.931283</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.491289</td>\n",
       "      <td>0.144239</td>\n",
       "      <td>38.861238</td>\n",
       "      <td>33.448872</td>\n",
       "      <td>-0.077841</td>\n",
       "      <td>0.921366</td>\n",
       "      <td>10.803862</td>\n",
       "      <td>16230396</td>\n",
       "      <td>16230396_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>398.452080</td>\n",
       "      <td>0.541613</td>\n",
       "      <td>0.271998</td>\n",
       "      <td>84.151871</td>\n",
       "      <td>340.348929</td>\n",
       "      <td>1.184020</td>\n",
       "      <td>-0.937548</td>\n",
       "      <td>83.618961</td>\n",
       "      <td>220.584748</td>\n",
       "      <td>-0.698928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.652512</td>\n",
       "      <td>0.383162</td>\n",
       "      <td>52.140729</td>\n",
       "      <td>36.279214</td>\n",
       "      <td>-0.759120</td>\n",
       "      <td>0.429387</td>\n",
       "      <td>18.971920</td>\n",
       "      <td>16230396</td>\n",
       "      <td>16230396_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>470.196750</td>\n",
       "      <td>-1.062051</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>274.789355</td>\n",
       "      <td>405.067821</td>\n",
       "      <td>-0.816950</td>\n",
       "      <td>-0.427824</td>\n",
       "      <td>118.126549</td>\n",
       "      <td>239.159817</td>\n",
       "      <td>-1.090345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869889</td>\n",
       "      <td>0.838840</td>\n",
       "      <td>49.336086</td>\n",
       "      <td>41.369661</td>\n",
       "      <td>-0.542480</td>\n",
       "      <td>0.600152</td>\n",
       "      <td>23.229765</td>\n",
       "      <td>16230396</td>\n",
       "      <td>16230396_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>477.783855</td>\n",
       "      <td>-1.361172</td>\n",
       "      <td>0.464569</td>\n",
       "      <td>187.479635</td>\n",
       "      <td>440.043544</td>\n",
       "      <td>-0.348134</td>\n",
       "      <td>0.646387</td>\n",
       "      <td>193.703996</td>\n",
       "      <td>213.095992</td>\n",
       "      <td>-0.924912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.310227</td>\n",
       "      <td>-0.108096</td>\n",
       "      <td>43.726065</td>\n",
       "      <td>24.439770</td>\n",
       "      <td>-1.112022</td>\n",
       "      <td>0.026026</td>\n",
       "      <td>7.912989</td>\n",
       "      <td>16230396</td>\n",
       "      <td>16230396_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417.306307</td>\n",
       "      <td>-0.713179</td>\n",
       "      <td>-0.619647</td>\n",
       "      <td>112.188014</td>\n",
       "      <td>335.667348</td>\n",
       "      <td>-1.215145</td>\n",
       "      <td>0.156117</td>\n",
       "      <td>140.693616</td>\n",
       "      <td>259.109848</td>\n",
       "      <td>-0.813610</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.112308</td>\n",
       "      <td>0.413483</td>\n",
       "      <td>59.538573</td>\n",
       "      <td>30.329723</td>\n",
       "      <td>0.805317</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>18.156995</td>\n",
       "      <td>16230396</td>\n",
       "      <td>16230396_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>251.772393</td>\n",
       "      <td>-1.091685</td>\n",
       "      <td>0.364251</td>\n",
       "      <td>76.597507</td>\n",
       "      <td>202.669652</td>\n",
       "      <td>-1.225869</td>\n",
       "      <td>0.018183</td>\n",
       "      <td>60.648647</td>\n",
       "      <td>156.296846</td>\n",
       "      <td>-0.514930</td>\n",
       "      <td>...</td>\n",
       "      <td>1.814169</td>\n",
       "      <td>0.607235</td>\n",
       "      <td>15.059297</td>\n",
       "      <td>27.059622</td>\n",
       "      <td>-1.049143</td>\n",
       "      <td>0.192883</td>\n",
       "      <td>10.078352</td>\n",
       "      <td>cf88d785</td>\n",
       "      <td>cf88d785_148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>1023.303423</td>\n",
       "      <td>-0.495014</td>\n",
       "      <td>-0.718650</td>\n",
       "      <td>253.663734</td>\n",
       "      <td>277.892588</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>1.150856</td>\n",
       "      <td>174.635574</td>\n",
       "      <td>209.978572</td>\n",
       "      <td>-0.966375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.907621</td>\n",
       "      <td>0.409998</td>\n",
       "      <td>32.088369</td>\n",
       "      <td>41.923898</td>\n",
       "      <td>1.176971</td>\n",
       "      <td>1.136630</td>\n",
       "      <td>16.815894</td>\n",
       "      <td>cf88d785</td>\n",
       "      <td>cf88d785_149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>719.141696</td>\n",
       "      <td>-1.255632</td>\n",
       "      <td>-0.059061</td>\n",
       "      <td>244.069346</td>\n",
       "      <td>364.504418</td>\n",
       "      <td>-1.544349</td>\n",
       "      <td>0.418076</td>\n",
       "      <td>274.720122</td>\n",
       "      <td>309.640371</td>\n",
       "      <td>-0.175616</td>\n",
       "      <td>...</td>\n",
       "      <td>3.814900</td>\n",
       "      <td>2.083426</td>\n",
       "      <td>71.966485</td>\n",
       "      <td>42.026060</td>\n",
       "      <td>0.105331</td>\n",
       "      <td>0.788962</td>\n",
       "      <td>17.016731</td>\n",
       "      <td>cf88d785</td>\n",
       "      <td>cf88d785_149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>1668.572680</td>\n",
       "      <td>-0.850939</td>\n",
       "      <td>0.966500</td>\n",
       "      <td>1563.982014</td>\n",
       "      <td>209.108127</td>\n",
       "      <td>-1.104097</td>\n",
       "      <td>-0.055342</td>\n",
       "      <td>105.655116</td>\n",
       "      <td>166.200306</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768537</td>\n",
       "      <td>0.546442</td>\n",
       "      <td>31.156542</td>\n",
       "      <td>80.495806</td>\n",
       "      <td>-1.087275</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>27.038209</td>\n",
       "      <td>cf88d785</td>\n",
       "      <td>cf88d785_149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>380.900422</td>\n",
       "      <td>-0.984965</td>\n",
       "      <td>-0.158604</td>\n",
       "      <td>61.949978</td>\n",
       "      <td>332.555694</td>\n",
       "      <td>-1.049128</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>162.840005</td>\n",
       "      <td>187.901475</td>\n",
       "      <td>-1.354753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985901</td>\n",
       "      <td>1.309855</td>\n",
       "      <td>43.127209</td>\n",
       "      <td>22.288491</td>\n",
       "      <td>-0.550446</td>\n",
       "      <td>0.341913</td>\n",
       "      <td>8.590431</td>\n",
       "      <td>cf88d785</td>\n",
       "      <td>cf88d785_149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2242 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ch_0_Delta-mean  ch_0_Delta-kurt  ch_0_Delta-skew  ch_0_Delta-std  \\\n",
       "0          390.720847        -1.310962         0.178668      152.372900   \n",
       "1          398.452080         0.541613         0.271998       84.151871   \n",
       "2          470.196750        -1.062051         0.770095      274.789355   \n",
       "3          477.783855        -1.361172         0.464569      187.479635   \n",
       "4          417.306307        -0.713179        -0.619647      112.188014   \n",
       "...               ...              ...              ...             ...   \n",
       "3348       251.772393        -1.091685         0.364251       76.597507   \n",
       "3349      1023.303423        -0.495014        -0.718650      253.663734   \n",
       "3350       719.141696        -1.255632        -0.059061      244.069346   \n",
       "3351      1668.572680        -0.850939         0.966500     1563.982014   \n",
       "3352       380.900422        -0.984965        -0.158604       61.949978   \n",
       "\n",
       "      ch_0_Theta-mean  ch_0_Theta-kurt  ch_0_Theta-skew  ch_0_Theta-std  \\\n",
       "0          428.479043        -0.226652        -0.017858      155.792028   \n",
       "1          340.348929         1.184020        -0.937548       83.618961   \n",
       "2          405.067821        -0.816950        -0.427824      118.126549   \n",
       "3          440.043544        -0.348134         0.646387      193.703996   \n",
       "4          335.667348        -1.215145         0.156117      140.693616   \n",
       "...               ...              ...              ...             ...   \n",
       "3348       202.669652        -1.225869         0.018183       60.648647   \n",
       "3349       277.892588        -0.008889         1.150856      174.635574   \n",
       "3350       364.504418        -1.544349         0.418076      274.720122   \n",
       "3351       209.108127        -1.104097        -0.055342      105.655116   \n",
       "3352       332.555694        -1.049128         0.239366      162.840005   \n",
       "\n",
       "      ch_0_Alpha-mean  ch_0_Alpha-kurt  ...  ch_3_Beta-kurt  ch_3_Beta-skew  \\\n",
       "0          253.743533        -0.931283  ...       -1.491289        0.144239   \n",
       "1          220.584748        -0.698928  ...       -0.652512        0.383162   \n",
       "2          239.159817        -1.090345  ...        0.869889        0.838840   \n",
       "3          213.095992        -0.924912  ...       -1.310227       -0.108096   \n",
       "4          259.109848        -0.813610  ...       -1.112308        0.413483   \n",
       "...               ...              ...  ...             ...             ...   \n",
       "3348       156.296846        -0.514930  ...        1.814169        0.607235   \n",
       "3349       209.978572        -0.966375  ...       -0.907621        0.409998   \n",
       "3350       309.640371        -0.175616  ...        3.814900        2.083426   \n",
       "3351       166.200306         0.023359  ...       -0.768537        0.546442   \n",
       "3352       187.901475        -1.354753  ...        0.985901        1.309855   \n",
       "\n",
       "      ch_3_Beta-std  ch_3_Gamma-mean  ch_3_Gamma-kurt  ch_3_Gamma-skew  \\\n",
       "0         38.861238        33.448872        -0.077841         0.921366   \n",
       "1         52.140729        36.279214        -0.759120         0.429387   \n",
       "2         49.336086        41.369661        -0.542480         0.600152   \n",
       "3         43.726065        24.439770        -1.112022         0.026026   \n",
       "4         59.538573        30.329723         0.805317         0.977456   \n",
       "...             ...              ...              ...              ...   \n",
       "3348      15.059297        27.059622        -1.049143         0.192883   \n",
       "3349      32.088369        41.923898         1.176971         1.136630   \n",
       "3350      71.966485        42.026060         0.105331         0.788962   \n",
       "3351      31.156542        80.495806        -1.087275         0.046721   \n",
       "3352      43.127209        22.288491        -0.550446         0.341913   \n",
       "\n",
       "      ch_3_Gamma-std       pid      group_id  workload  \n",
       "0          10.803862  16230396    16230396_1         0  \n",
       "1          18.971920  16230396    16230396_2         0  \n",
       "2          23.229765  16230396    16230396_3         0  \n",
       "3           7.912989  16230396    16230396_3         0  \n",
       "4          18.156995  16230396    16230396_4         0  \n",
       "...              ...       ...           ...       ...  \n",
       "3348       10.078352  cf88d785  cf88d785_148         1  \n",
       "3349       16.815894  cf88d785  cf88d785_149         1  \n",
       "3350       17.016731  cf88d785  cf88d785_149         1  \n",
       "3351       27.038209  cf88d785  cf88d785_149         1  \n",
       "3352        8.590431  cf88d785  cf88d785_149         1  \n",
       "\n",
       "[2242 rows x 83 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_df = pd.read_csv('CHESS_notch_bp_avg_mastoid_annotated_ica_fft.csv')\n",
    "chess_df['group_id'] = chess_df['pid'] + '_' + chess_df['blockid'].astype(str)\n",
    "chess_df = chess_df[(chess_df['elo-bin'] == 'lo') | (chess_df['elo-bin'] == 'hi')]\n",
    "chess_df['workload'] = chess_df['elo-bin'].apply(lambda x: 0 if x == 'lo' else 1)\n",
    "chess_df = chess_df.drop(columns=['elo', 'solved', 'blockid', 'sub-block-id', 'elo-bin'])\n",
    "chess_freq = produce_freq_bands(chess_df)\n",
    "\n",
    "print(chess_freq.shape)\n",
    "chess_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2137, 80) - (2137,) - (796, 80) - (796,)\n",
      "[LightGBM] [Info] Number of positive: 1162, number of negative: 975\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20400\n",
      "[LightGBM] [Info] Number of data points in the train set: 2137, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.543753 -> initscore=0.175460\n",
      "[LightGBM] [Info] Start training from score 0.175460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrussell/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc         0.652010\n",
      "log_reg     0.594902\n",
      "rf          0.827889\n",
      "knn         0.660770\n",
      "ann         0.752371\n",
      "gbm         0.738414\n",
      "lightgbm    0.817837\n",
      "xgboost     0.821604\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_freq_data = pd.concat([chess_freq, wl_freq])  #wl_freq #\n",
    "X_train, X_test, y_train, y_test = split_data(all_freq_data)\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train, X_test)\n",
    "results = classify(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data  |      Classifier      | macro f1 |\n",
    "|:--------:|:----------------------:|:-----------:|\n",
    "| Nback\t       | lightgbm  |    0.82 |\n",
    "| Nback + Rotation | rf | 0.76 |\n",
    "| Chess + Nback |   rf      |    0.75 |\n",
    "| Nback + Stroop | rf | 0.72 |\n",
    "| Nback + Stroop + Rotation\t       |   rf\t     | 0.67 |\n",
    "| All          |   rf      |    0.67 |\n",
    "| Chess + Rotation | rf | 0.64 |\n",
    "| Rotation\t   | xgboost\t  |    0.63 |\n",
    "| Nback (4-class) | lightgbm | 0.62 |\n",
    "| Stroop\t   |   rf\t     | 0.62 |\n",
    "| Chess        |   rf      |    0.62 |\n",
    "| Chess + Rotation + Stroop | rf | 0.62 |\n",
    "| Chess + Stroop | rf | 0.62 |\n",
    "| Stroop + Rotation | rf | 0.59 |\n",
    "| Chess + Nback + Stroop | rf | 0.57\n",
    "| Chess + Nback + Rotation | rf | 0.55 |\n",
    "\n",
    "\n",
    "with added kurtosis, skew, and std\n",
    "| Data  |      Classifier      | macro f1 |\n",
    "|:--------:|:----------------------:|:-----------:|\n",
    "| Nback\t       | rf |    0.83 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def produce_freq_bands(df):\n",
    "\n",
    "    # Define the frequency bands\n",
    "    freq_bands = {\n",
    "        \"Delta\": (0.5, 4),\n",
    "        \"Theta\": (4, 8),\n",
    "        \"Alpha\": (8, 14),\n",
    "        \"Beta\":  (14, 30),\n",
    "        \"Gamma\": (30, 45)\n",
    "    }\n",
    "\n",
    "    # Extract only the columns with frequency data\n",
    "    freq_data = df.filter(like=\"freq\")\n",
    "\n",
    "    binned_df = pd.DataFrame()\n",
    "\n",
    "    for ch in range(4):  # Assuming you have 4 channels: ch_0, ch_1, ch_2, ch_3\n",
    "        for band, (f_min, f_max) in freq_bands.items():\n",
    "            # Filter columns for the current channel and band\n",
    "            cols = [col for col in freq_data.columns \n",
    "                    if f\"ch_{ch}_freq_\" in col \n",
    "                    and f_min <= float(col.split(\"_\")[3][:-2]) <= f_max]\n",
    "            \n",
    "            # Aggregate the columns and add to the binned dataframe\n",
    "            if cols:\n",
    "                binned_df[f\"ch_{ch}_{band}-mean\"] = freq_data[cols].mean(axis=1)\n",
    "                binned_df[f\"ch_{ch}_{band}-kurt\"] = freq_data[cols].kurt(axis=1)\n",
    "                binned_df[f\"ch_{ch}_{band}-skew\"] = freq_data[cols].skew(axis=1)\n",
    "                binned_df[f\"ch_{ch}_{band}-std\"] = freq_data[cols].std(axis=1)\n",
    "            else:\n",
    "                binned_df[f\"ch_{ch}_{band}\"] = np.nan\n",
    "\n",
    "    # Add back non-frequency columns to the binned dataframe\n",
    "    for col in [key for key in df.keys() if key not in freq_data]:\n",
    "        binned_df[col] = df[col]\n",
    "\n",
    "    df = binned_df\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "within subjects test; make sure to separate data into EITHER test/train based on which file produced the data. technically could get more specific with nback task, but this is generalizable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "def classify(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    models = {  \n",
    "                # 'svc': SVC(), \n",
    "                # 'log_reg': LogisticRegression(max_iter=10000), \n",
    "                'rf': RandomForestClassifier(), \n",
    "                'knn': KNeighborsClassifier(n_neighbors=5),  # Start with 5 neighbors, \n",
    "                'ann': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=10000),  # Simple 2-layer feedforward network,\n",
    "                'gbm': GradientBoostingClassifier(), \n",
    "                'lightgbm': lgb.LGBMClassifier(), \n",
    "                'xgboost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "            }\n",
    "\n",
    "    results = {model: None for model in models.keys()}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')    \n",
    "        results[model_name] = f1\n",
    "\n",
    "    results = pd.Series(results)\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
