---
title: 'Analysis of EEG data'
author: 'Matthew Russell'
output: html_document
---

# http://jordanjamesbird.com/publications/A-Study-on-Mental-State-Classification-using-EEG-based-Brain-Machine-Interface.pdf
# https://www.researchgate.net/publication/331733067_A_Deep_Evolutionary_Approach_to_Bioinspired_Classifier_Optimisation_for_Brain-Machine_Interaction 
# https://www.nature.com/articles/s41598-019-42192-z - tempoparietal tdcs & AHA
# check out for ML (above)

# (TODO: PPG)
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7293675/

# load libraries
```{r}
library(doParallel)
library(dplyr)
library(tidyr)
library(lme4)
library(brms)
library(broom)
library(emmeans)
library(broom.mixed)
library(rstatix)
library(caret)
library(caTools)
library(r2r)
library(purrr)
library(ez)
```

```{r}
set.seed(1)

keys_to_drop <- c("STATUS", "BLOCK_TYPE", 'timestamp', 'TRIAL_TYPE', 'TASK_NAME')

add_stars <- function(df) {
    # Add a new column with significance stars
    df <- df %>%
    mutate(adj..signif = case_when(
        adj.p.value <= 0.001 ~ "***",
        adj.p.value <= 0.01 ~ "**",
        adj.p.value <= 0.05 ~ "*",
        adj.p.value <= 0.1 ~ ".",
        TRUE ~ ""
    ))
    return(df)
}
```

# load data
```{r}
loaded_df <- read.csv("mw_study_p_5_2_psd_bp_notch_smooth_avgref.csv")

orig_df <- loaded_df

# drop the extra metadata
orig_df <- orig_df %>% select(-c(keys_to_drop))

orig_df$CORRECT[orig_df$CORRECT == "true"] <- 1
orig_df$CORRECT[orig_df$CORRECT != 1] <- 0
orig_df$CORRECT <- as.numeric(orig_df$CORRECT)

# going to 'average' same participant, might as well make it numeric
orig_df$pid <- as.factor(orig_df$pid)

# each p_id contains multiple block_ids for same task type, so group.
orig_df <- orig_df %>% group_by(filename, pid, task) %>% summarize_all(mean) %>% ungroup()
```

# anova
```{r}
df <- orig_df 
df <- df %>% select(-c(CORRECT))
df_long <- df %>% pivot_longer(starts_with("delta") | starts_with("theta") | starts_with("alpha") | starts_with("beta") | starts_with("gamma"),
    names_to = c("frequency", "channel"),
    names_pattern = "(.*)\\.samples_(.*)",
    values_to = "value"
  )

nback_data <- df_long %>% filter(grepl("nback", task))
nback_data <- nback_data %>% filter(task == "nback_0_task" | task == "nback_2_task")
anova_result <- ezANOVA(dv = value, wid = pid, within = c(task, frequency), data=nback_data)

```

# OUTLIER DETECTION AND REMOVAL
```{r}
grouped <- orig_df


outliers <- sapply(colnames(grouped), function(col) {
    if (col == "pid" || col == "filename" || col == "task" || col == "block_id" || col == "CORRECT") {
        return(FALSE)
    }
    print(col)
    grouped %>% select(c(pid, filename, block_id, CORRECT, task, col)) %>%
                identify_outliers(col) %>%
                filter(is.extreme == TRUE)
})

# remove all gms that are extreme outliers.
outlier_unique_ids <- sapply(outliers, function(outlier_df) {
    tryCatch(

        return(unique(outlier_df$unique_id)),
        error = function(e) {  # What to do when an error occurs
            return(c())
        }
    )
})
flat_outlier_unique_ids <- unique(unlist(outlier_unique_ids))

grouped <- grouped %>% filter(!(unique_id %in% flat_outlier_unique_ids))
```

# Label Data
```{r}
# sumry <- grouped %>% group_by(pid) %>% summarize(lower = quantile(elo, 0.25),
#                                                  upper = quantile(elo, 0.75))
# # for each pid, i'd like to mark each puzzle as 0, 1, or 2, where 0 is any puzzle
# # less than or equal to the 25th percentile, 1 is between the 25th and 75th percentile,
# # and 2 is greater than or equal to the 75th percentile.

# # Join the 'grouped' and 'sumry' data frames
# grouped_with_sumry <- grouped %>% left_join(sumry, by = "pid")

# # Create a new column based on the elo values and the quartiles
# grouped_with_new_col <- grouped_with_sumry %>% mutate(
#   elo_category = case_when(
#     elo <= lower ~ 0,
#     elo > lower & elo < upper ~ 2,
#     elo >= upper ~ 1
#   )
# )

grouped$evaluation <- grouped_with_new_col$elo_category

grouped <- grouped %>% select(-c(elo, unique_id))

# grouped$evaluation <- grouped$solved
grouped_lohi <- grouped
#grouped_t <- grouped %>% filter(solved == 1)
#grouped_lohi <- grouped_t %>% filter(evaluation == 0 | evaluation == 2)
#disc_df <- orig_df %>% filter(evaluation <= MAX_BAD | evaluation >= MIN_GOOD)
```

# Format Data
```{r}


lohi <- orig_df %>% filter(task == "nback_2_task" | task == "nback_0_task")

# turn all of the columns from wavelength.probe to a long format for stats
# Gather all the columns except for "evaluation", "pnum", and 'gmnum'
# into a "long" format, with a new column called "variable"
# that contains the original column names
long_df <- lohi %>%
    gather(variable, value, -evaluation, -pid, -solved, -block_id) %>%
    separate(variable, c("wavelength", "probe"), sep = "\\.", extra = "merge")

long_df$pid <- as.factor(long_df$pid)
long_df$wavelength <- as.factor(long_df$wavelength)
long_df$probe <- as.factor(long_df$probe)

```

# ANOVA
```{r}
long_df$evaluation <- as.factor(long_df$evaluation)
anova_df <- long_df %>% select(-c(block_id, solved))

# Check the normality of the differences between the scores
# for each combination of levels of the factors 'evaluation' and 'wavelength'.

# Group by 'pnum', 'evaluation', and 'wavelength', and calculate the mean 'value' for each group
anova_df_grouped <- anova_df %>%
  group_by(pid, evaluation, wavelength) %>%
  summarise(mean_value = mean(value, na.rm = TRUE))

# Calculate the differences between the scores for each combination of levels of the factors
data_diff <- anova_df_grouped %>%
  pivot_wider(names_from = evaluation, values_from = mean_value) %>%
  group_by(pid, wavelength) %>%
  mutate(diff = `1` - `0`)

# Check the normality for each combination of levels of the factors
normality_tests <- data_diff %>%
  group_by(wavelength) %>%
  nest() %>%
  mutate(shapiro_test = map(data, ~ shapiro.test(.x$diff)),
         W_statistic = map_dbl(shapiro_test, "statistic"),
         p_value = map_dbl(shapiro_test, "p.value")) %>%
  select(wavelength, W_statistic, p_value)



# Print the Shapiro-Wilk test results
print(normality_tests)

# option 2: mean of all values for each wavelength for each gmnum
anova_df <- long_df %>% group_by(pid, wavelength, evaluation, block_id) %>%
                     summarise(value = mean(value)) %>%
                     ungroup() #%>%
                     #pivot_wider(names_from = "wavelength")
anova_df <- filter(anova_df, evaluation == 0 | evaluation == 1)

#anova_df %>% group_by(wavelength) %>% 



# assumption of normality for gamma is violated. 
# let's use a friedman test instead.

#anova_df <- anova_df %>% select(-c(elo, block_id, unique_id, solved))


run_anova <- function(which_way, anova_df) {
    if (which_way == "two-way") {
        rex <- "evaluation(0) (alpha|beta|delta|gamma|theta) - evaluation(1) \\2" #nolint
        ctrast_str <- "\\2 \\1-\\3"
        term_str <- "eval*wave"
        anova_res <- ezANOVA(
                        data = anova_df,
                        dv = value,
                        wid = pid,
                        within = c(evaluation, wavelength),
                        within_full = c(evaluation, wavelength),
                        type = 2
                    )
        model <- lmer(value ~ evaluation * wavelength + (1 | pid), data = anova_df)
        emm <- emmeans(model, ~ evaluation * wavelength)
    }else {
        rex <- "evaluation(0) (alpha|beta|delta|gamma|theta) (AF7|AF8|TP9|TP10) - evaluation(1) \\2 \\3" #nolint
        ctrast_str <- "\\2 \\3 \\1-\\4"
        term_str <- "e*w*p"
        anova_res <- ezANOVA(
                        data = anova_df,
                        dv = value,
                        wid = pnum,
                        within = c(evaluation, wavelength, probe),
                        within_full = c(evaluation, wavelength, probe),
                        type = 2 
                    )
        model <- lmer(value ~ evaluation * wavelength * probe + (1 | pid),
                     data = anova_df)
        emm <- emmeans(model, ~ evaluation * wavelength * probe)
    }
    print(anova_res)
    pairwise_comparisons <- pairs(emm, adjust = "bonferroni")
    tpwc <- tidy(pairwise_comparisons)

    filt <- tpwc[grep(rex, tpwc$contrast), ]
    filt$term <- term_str
    filt$contrast <- gsub(rex, ctrast_str, filt$contrast)
    filt <- add_stars(filt)
    filt
}
run_anova("two-way", anova_df)
run_anova("three-way", anova_df)
```


# GLMER 
```{r}
total <- sum(long_df$evaluation == 0) + sum(long_df$evaluation == 1)
w0 <- 1 / (sum(long_df$evaluation == 0) / total)

mod_glm <- glmer(evaluation ~ value:wavelength + (1 | pid),
                family = binomial(link = "logit"),
                data = long_df,
                weights = ifelse(long_df$evaluation == 0, w0, 1))
mod_glm_df <- tidy(mod_glm)
mod_glm_df$adj.p.value <- p.adjust(mod_glm_df$p.value, method = "bonferroni")
mod_glm_df <- add_stars(mod_glm_df)
mod_glm_df

mod_glm_no_random <- glm(evaluation ~ value:wavelength, 
                         family = binomial(link = "logit"),
                         data = long_df,
                         weights = ifelse(long_df$evaluation == 0, w0, 1))

anova(mod_glm_no_random, mod_glm, test = "LRT")
```


# BRM
```{r}
mod_brm <- brm(evaluation ~ value:wavelength + (1 | pnum),
                family = bernoulli(link = "logit"),
                data = disc_df,
                opencl = opencl(c(0, 0)),
                cores = 18,
                backend = "cmdstanr")
```

# Machine Learning
```{r}

#TODO: Tune grids with training, validation, test sets.


#option 1: use disc_df values AF7.alpha ... TP10.theta
ml_df <- lohi %>% select(-c(CORRECT)) %>% ungroup()

# #option 2: AF7.alpha ... TP10.theta ... delta_theta_AF8 ... delta_theta_TP10
# for (i in seq_along(wavelengths)) {
#   for (j in seq_along(wavelengths)[-seq_len(i)]) {
#     wave1 <- wavelengths[i]
#     wave2 <- wavelengths[j]
#     for (probe in probes) {
#       int_var <- paste0(wave1, "_", wave2, "_", probe)
#       ml_df[, int_var] <- ml_df[, paste0(wave1, ".", probe)] / ml_df[, paste0(wave2, ".", probe)]
#     }
#   }
# }

# option 2: mean of all values for each wavelength for each gmnum
ml_df <- long_df %>% group_by(pid, wavelength, evaluation, block_id) %>%
                     summarise(value = mean(value)) %>%
                     ungroup() %>%
                     pivot_wider(names_from = "wavelength")

# # option 3: add the ratio data for each pair of wavelengths
# ml_df <- ml_df %>%
#          mutate(
#                 alpha_beta = alpha - beta,
#                 alpha_delta = alpha - delta,
#                 alpha_gamma = alpha - gamma,
#                 alpha_theta = alpha - theta,
#                 beta_delta = beta - delta,
#                 beta_gamma = beta - gamma,
#                 beta_theta = beta - theta,
#                 delta_gamma = delta - gamma,
#                 delta_theta = delta - theta,
#                 gamma_theta = gamma - theta
#                 )
#ml_df$pnum <- as.character(ml_df$pnum)
ml_df <- lohi %>% select(-c(CORRECT)) %>% ungroup()
ml_df$task <- make.names(as.factor(ml_df$task))

# 3 repeats of 10-fold cross validation for training data
# upsample data to balance classes
trnCtrl <- trainControl(
                        method = "repeatedcv",
                        repeats = 3,
                        classProbs = TRUE,
                        sampling = "up"
                        )

runModel <- function(method, data, ctrl = trnCtrl) {

    sample <- sample.split(data$task, SplitRatio = 0.8)
    train_data  <- subset(data, sample == TRUE)
    test_data   <- subset(data, sample == FALSE)

    model <- train(task ~ .,
                    data = train_data,
                    method = method,
                    trControl = ctrl,
                    metric = "Accuracy",
                    tuneLength = 10)

    pred <- predict(model, newdata = test_data)
    actual <- as.factor(make.names(as.factor(test_data$task)))
    cMat <- confusionMatrix(data = pred, reference = actual)
    return(cMat)
}
```
# Run the models    

```{r}
library(doParallel)
cl <- makePSOCKcluster(ifelse(Sys.info()["sysname"] == "Linux", 4, 4)) #18, 5))
registerDoParallel(cl)

# dead_models: "nnet", what's up with qda?? "lda",
modelNames <- c( "svmLinear", "svmRadial", "rf", "knn",
                "naive_bayes", "glmnet") #, "adaboost", "gbm", "xgbTree")

accuracy_results <- tibble(
  pid = character(),
  model = character(),
  accuracy = double()
)

for (p in unique(ml_df$pid)) {
    print(paste("pid", p))
    for (modelName in modelNames) {
        p_data <- ml_df %>% filter(pid == p) %>% select(-c(pid, filename, block_id))
        res <- runModel(modelName, p_data)
        acc <- res$byClass["Balanced Accuracy"]
        accuracy_results <- accuracy_results %>%
        add_row(pid = p, model = modelName, accuracy = acc)
    }
}

# # Pivot the long format data frame to a wide format
ml_results <- accuracy_results %>% spread(key = model, value = accuracy) %>% as.data.frame()

ml_results <- ml_results %>% select(-c(pnum))
print(ml_results)

print(rowMeans(ml_results))
print(colMeans(ml_results))

# run the models on all particpant data
global_res <- tibble(
  model = character(),
  accuracy = double()
)
for (modelName in modelNames) {
    print(modelName)
    res <- runModel(modelName, ml_df %>% select(-c(pid, block_id)))
    acc <- res$byClass["Balanced Accuracy"]
    global_res <- global_res %>%
    add_row(model = modelName, accuracy = acc)
}
# # Pivot the long format data frame to a wide format
gl_ml_res <- global_res %>% spread(key = model, value = accuracy) %>%
                            as.data.frame()
gl_ml_res
```